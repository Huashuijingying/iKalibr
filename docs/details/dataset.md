<div align=center><img src="../img/logo.svg" width =100%></div>

---

<h3 align="center">Access: Dataset Utilized in iKalibr for Evaluation</h3>
<p align="center">
    <a href="https://github.com/Unsigned-Long"><strong>Author » Shuolong Chen</strong></a>
</p>


---

<p align="left">
    <a><strong>Pre-Instructions (You Have To Read This) »</strong></a>
</p> 




All the datasets mentioned below have been tested in `iKalibr`, and the corresponding configuration files can be found in the folder of the same name under the data folder (`ikalibr/data`) of the code repository. To get a taste of `iKalibr` for the first time, you only need to download the corresponding dataset and slightly modify the configuration file we uploaded (mainly the file path) to perform calibration.

If you want to try camera/IMU calibration in `iKalibr`, we strongly recommend that you use the TUM GS-RS dataset instead of our dataset. The reasons are:

+ our dataset only has GS cameras, not RS cameras;
+ the image quality in our dataset is very poor (there is motion blur, caused by our carelessness in setting the exposure time).

At the same time, considering that it takes a certain amount of time to use `colmap/glomap` for SfM (required when `iKalibr` calibrates the camera), we have also uploaded the `colmap` SfM result file of the camera in the corresponding dataset, which you can get from [here](https://1drv.ms/f/c/9a25f256101134a4/EoAnE8FD9vFDqgQkSGffl8gBJIEBHDxASc_PsKhDOilVsw?e=Quxjf7) (for both `iKalibr` dataset and `TUM GS-RS` dataset). When using these files, just put the corresponding three files (`cameras.txt`, `images.txt`, and `points3D.txt`) into the folder (named `sfm_ws`) that `iKalibr` created for you to solve SfM. 

Of course, if you want to calibrate your own camera, you will need to use `colmap` for SfM (the command lines adapted for `colmap` generated by `iKalibr` is in the file `sfm-command-line.txt`). This may take about 10 to 20 minutes (for a 10Hz 1024x768 image sequence of less than one minute).

**Attention**: For RGBD cameras, no SfM is required for spatiotemporal calibration in `iKalibr`!

<p align="left">
    <a><strong>Our Dataset »</strong></a>
</p> 



The platform integrates two FLIR cameras (denoted as `Cam-0` and `Cam-1`), a Velodyne VLP-32C LiDAR (denoted as `LiDAR-0`), a Livox Avia LiDAR (denoted as `LiDAR-1`), two TI AWR1843 3D Radars (denoted as `Rad-0` and `Rad-1`), a MTI-G-710 MEMS IMU (denoted as `IMU-0`), and a Livox Avia build-in MEMS IMU (denoted as `IMU-1`). The acquisition rate for cameras, LiDARs, and radars is set to 10 Hz, as for `IMU-0` and `IMU-1`, they are set to 400 Hz and 200 Hz, respectively. All sensors are only software-synchronized, thus unknown time offsets exist, whose determination would be considered in calibration. 

You can find the corresponding origin data [here](https://drive.google.com/drive/folders/1n0OhdbOgirg4Sw-p1YjS4bVPqje3UEnN?usp=sharing).

<p align="left">
    <a><strong>LI-Calib (OA-Calib) Dataset »</strong></a>
</p> 



This dataset contains ten data sequences collected handheld in both indoor (garage) and outdoor (court) scenarios.
The self-assembled sensor rig integrates three Xsens-100 IMUs (denoted as `IMU-0`, `IMU-1`, and `IMU-2`) sampled at
400 Hz and a Velodyne VLP-16 LiDAR sampled at 10 Hz. Sensors except the LiDAR are hardware-synchronized.

You can find the corresponding origin data [here](https://drive.google.com/drive/folders/1kYLVLMlwchBsjAoNqnrwq2N2Ow5na4VD?usp=sharing).

<p align="left">
    <a><strong>River Dataset »</strong></a>
</p> 



This dataset contains three data sequences collected handheld in indoor scenario. The self-assembled sensor rig integrates two AWR1843BOOST 3D radars (denoted as `Rad-0` and `Rad-1`) sampled at 10 Hz and an XSens MTI-G-710 MESE IMU sampled at 400 Hz. Sensors are only software-synchronized.

You can find the corresponding origin data [here](https://drive.google.com/drive/folders/1olNgh9i_lmJ96gZB6oxYUSqACBpOzY7u?usp=sharing).

<p align="left">
    <a><strong>TUM GS-RS Dataset »</strong></a>
</p> 




This dataset contains ten data sequences collected handheld in the indoor scenarios. The self-assembled sensor rig integrates two uEye UI-3241LE-M-GL cameras (denoted as `Cam-0` and `Cam-1`) sampled at 20 Hz and a Bosch BMI160 IMU sampled at 200 Hz. The `Cam-0` runs in GS mode and the `Cam-1` in RS mode. Sensors are hardware-synchronized.

You can find the corresponding origin data [here](https://cvg.cit.tum.de/data/datasets/rolling-shutter-dataset).



<p align="left">
    <a><strong>VECtor Dataset »</strong></a>
</p> 



This dataset is captured by a sensor suite that includes an event stereo camera, a regular stereo camera, an RGB-D sensor, a LiDAR, and an IMU. Since its authors provide a [depth alignment tool](https://github.com/greatoyster/k4a_projector.git), you can get depth images consistent with any camera, which thus can be regarded as an RGBD camera.

You can find the corresponding origin data [here](https://star-datasets.github.io/vector).
